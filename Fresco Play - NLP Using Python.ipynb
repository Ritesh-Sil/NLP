{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dba46ee",
   "metadata": {},
   "source": [
    "Text Corpora\n",
    "In the previous topic, you have worked with a simple text collection text1.\n",
    "\n",
    "In this topic, you will work with larger text collections known as Text Corpora or Text Corpus.\n",
    "\n",
    "The following code snippet downloads more text corpus, which is varied in content.\n",
    "\n",
    ">>> import nltk\n",
    ">>> nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed2555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b484ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08033ef8",
   "metadata": {},
   "source": [
    "Popular Text Corpora\n",
    "Two popular Text Corpora available from nltk, which you will be using in this course are:\n",
    "\n",
    "Genesis: It is a collection of few words across multiple languages.\n",
    "\n",
    "Brown: It is the first electronic corpus of one million English words.\n",
    "\n",
    "Other Corpus in nltk\n",
    "\n",
    "Gutenberg : Collections from Project Gutenberg\n",
    "Inaugural : Collection of U.S Presidents inaugural speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b64b2",
   "metadata": {},
   "source": [
    "Popular Text Corpora\n",
    "stopwords : Collection of stop words.\n",
    "reuters : Collection of news articles.\n",
    "cmudict : Collection of CMU Dictionary words.\n",
    "movie_reviews : Collection of Movie Reviews.\n",
    "np_chat : Collection of chat text.\n",
    "names : Collection of names associated with males and females.\n",
    "state_union : Collection of state union address.\n",
    "wordnet : Collection of all lexical entries.\n",
    "words : Collection of words in Wordlist corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6a814f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (398473770.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Accessing Text Corpora\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Accessing Text Corpora\n",
    "Any text corpus has to be imported before you start working with it.\n",
    "\n",
    "The below code imports genesis text corpus.\n",
    "\n",
    ">>> from nltk.corpus import genesis\n",
    "Various text collections available under genesis text corpus are viewed by fileids method.\n",
    ">>> genesis.fileids()\n",
    "['english-kjv.txt',\n",
    " 'english-web.txt',\n",
    "  ....]\n",
    "The output displays eight text collections, present in genesis corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58cf8711",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (197967404.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Working with a Text Corpus\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Working with a Text Corpus\n",
    "Now let's understand how to work with a text corpus.\n",
    "\n",
    "The following example determines the average word length and average sentence length of each text collection present in genesis corpus.\n",
    "\n",
    ">>> for fileid in genesis.fileids():\n",
    "...    n_chars = len(genesis.raw(fileid))\n",
    "...    n_words = len(genesis.words(fileid))\n",
    "...    n_sents = len(genesis.sents(fileid))\n",
    "...    print(int(n_chars/n_words), int(n_words/n_sents), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609a4821",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2437675222.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [7]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Working with a Text Corpus\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Working with a Text Corpus\n",
    "The methods raw, words and sents used in code determine the total number of characters, words, and sentences present in a specific text collection.\n",
    "\n",
    "The output of code is shown below. Text collection finnish.txt has different average word length.\n",
    "\n",
    "4 30 english-kjv.txt\n",
    "4 19 english-web.txt\n",
    "5 15 finnish.txt\n",
    "4 23 french.txt\n",
    "4 23 german.txt\n",
    "4 20 lolcat.txt\n",
    "4 27 portuguese.txt\n",
    "4 30 swedish.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bc0f65a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3456525732.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [8]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Text Corpus Structure\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Text Corpus Structure\n",
    "A text corpus is organized into any of the following four structures.\n",
    "\n",
    "Isolated - Holds Individual text collections.\n",
    "\n",
    "Categorized - Each text collection tagged to a category.\n",
    "\n",
    "Overlapping - Each text collection tagged to one or more categories, and\n",
    "\n",
    "Temporal - Each text collection tagged to a period, date, time, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb1bed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3497475574.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [9]\u001b[1;36m\u001b[0m\n\u001b[1;33m    genesis text corpus has eight text collections, which are isolated in structure.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "genesis text corpus has eight text collections, which are isolated in structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f288b754",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3990246265.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [10]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Each text collection is tagged to a specific category or genre.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Each text collection is tagged to a specific category or genre.\n",
    "\n",
    "E.g.: Brown text corpus contains 500 collections, which are categorized into 15 genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b42bce",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4242438840.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [11]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Each collection is categorized into one or more genre.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Each collection is categorized into one or more genre.\n",
    "\n",
    "E.g.: Reuters corpus contains 10788 collections, which are tagged to 90 genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4453c6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1466018950.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [13]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ach text collection is tagged to a period of time.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ach text collection is tagged to a period of time.\n",
    "\n",
    "E.g.: inaugural corpus contain text collections corresponding to U.S inaugural presidential speeches, \n",
    "    gathered over a period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e6fbb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1577865863.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [14]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Loading User Specific Corpus\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Loading User Specific Corpus\n",
    "Now let's see how to convert your collection of text files into a text corpus.\n",
    "\n",
    "Suppose, you have three files c1.txt, c2.txt and c3.txt in /usr/home/dict path.\n",
    "\n",
    "Creation of corpus wordlists corpus is shown in the following example.\n",
    "\n",
    ">>> from nltk.corpus import PlaintextCorpusReader\n",
    ">>> corpus_root = '/usr/share/dict'\n",
    ">>> wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    ">>> wordlists.fileids()\n",
    "['c1.txt',\n",
    " 'c2.txt',\n",
    " 'c3.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7279d",
   "metadata": {},
   "source": [
    "#### Conditional-Frequency-Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16730be7",
   "metadata": {},
   "source": [
    "Conditional Frequency\n",
    "In the previous topic, you have studied about Frequency Distributions.\n",
    "\n",
    "FreqDist function computes the frequency of each item in a list.\n",
    "\n",
    "While computing a frequency distribution, you observe occurrence count of an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c32c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']\n",
    "nltk.FreqDist(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a654b7",
   "metadata": {},
   "source": [
    "A Conditional Frequency is a collection of frequency distributions, computed based on a condition.\n",
    "\n",
    "For computing a conditional frequency, you have to attach a condition to every occurrence of an event.\n",
    "\n",
    "Let's consider the following list for computing Conditional Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e434d",
   "metadata": {},
   "source": [
    "Each item is grouped either as a fruit F or a vegetable V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a55f871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaece495",
   "metadata": {},
   "source": [
    "Computing Conditional Frequency\n",
    "ConditionalFreqDist function of nltk is used to compute Conditional Frequency Distribution (CDF).\n",
    "\n",
    "The same can be viewed in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563fa7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'V']\n",
      "<FreqDist with 2 samples and 3 outcomes>\n",
      "<FreqDist with 2 samples and 3 outcomes>\n"
     ]
    }
   ],
   "source": [
    "cfd=nltk.ConditionalFreqDist(c_items)\n",
    "print(cfd.conditions())\n",
    "print(cfd['V'])\n",
    "print(cfd['F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d74d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ddaf548",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3024622302.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [24]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Counting Words by Genre\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Counting Words by Genre\n",
    "Now let's determine the frequency of words, of a particular genre, in brown corpus.\n",
    ">>> cfd = nltk.ConditionalFreqDist([ \n",
    "(genre, word) \n",
    "for genre in brown.categories() \n",
    "for word in brown.words(categories=genre) ])\n",
    "The conditions applied can be viewed as shown below.\n",
    ">>> cfd.conditions()\n",
    "['adventure',\n",
    " 'hobbies',\n",
    " ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6410bdac",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:8\u001b[1;36m\u001b[0m\n\u001b[1;33m    reviews         14        1         2\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Viewing Word Count\n",
    "Once after computing conditional frequency distribution, tabulate method is used for viewing the count along with arguments conditions and samples.\n",
    ">>> cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])\n",
    "\n",
    "           leadership  worship  hardship \n",
    "government         12        3         2 \n",
    "     humor          1        0         0 \n",
    "   reviews         14        1         2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44d3e4f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:8\u001b[1;36m\u001b[0m\n\u001b[1;33m    reviews         14       15        17\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Viewing Cumulative Word Count\n",
    "The cumulative count for different conditions is found by setting cumulative argument value to True.\n",
    ">>> cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)\n",
    "\n",
    "           leadership   worship  hardship \n",
    "government         12       15        17 \n",
    "     humor          1        1         1 \n",
    "   reviews         14       15        17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3380aeda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3178981813.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [29]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Accessing Individual Frequency Distributions\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Accessing Individual Frequency Distributions\n",
    "From the obtained conditional frequency distribution, you can access individual frequency distributions.\n",
    "\n",
    "The below example extracts frequency distribution of words present in news genre of brown corpus.\n",
    "\n",
    ">>> news_fd = cfd['news']\n",
    ">>> news_fd.most_common(3)\n",
    "[('the', 5580), (',', 5188), ('.', 4030)]\n",
    "You can further access count of any sample as shown below.\n",
    ">>> news_fd['the']\n",
    "5580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59578a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4167766303.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [30]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Comparing Frequency Distributions\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Comparing Frequency Distributions\n",
    "Now let's see another example, which computes the frequency of last character appearing in all names associated with males and females respectively and compares them.\n",
    "\n",
    "The text corpus names contain two files male.txt and female.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8851ff33",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2037564955.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [31]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Comparing Frequency Distributions\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Comparing Frequency Distributions\n",
    ">>> from nltk.corpus import names\n",
    ">>> nt = [(fid.split('.')[0], name[-1])    for fid in names.fileids() \n",
    "  for name in names.words(fid) ]\n",
    ">>> cfd2 = nltk.ConditionalFreqDist(nt)\n",
    ">>> sum([cfd2['female'][x] for x in cfd2['female']]) > sum([cfd2['male'][x] for x in cfd2['male']])\n",
    "True\n",
    "The expression sum([cfd2['female'][x] for x in cfd2['female']]) > sum([cfd2['male'][x] for x in cfd2['male']]) checks if the last characters in females occur more frequently than the last characters in males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08979b2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (125039170.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [32]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Comparing Frequency Distributions\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Comparing Frequency Distributions\n",
    "The following code snippet displays frequency count of characters a and e in females and males, respectively.\n",
    ">>> cfd2.tabulate(samples=['a', 'e'])\n",
    "          a    e \n",
    "female 1773 1432 \n",
    "  male   29  468 \n",
    "You can observe a significant difference in frequencies of a and e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ca39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e172d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b680601",
   "metadata": {},
   "source": [
    "# Bigrams,Ngrams & Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91b3b222",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2776283413.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [33]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Bigrams represent a set of two consecutive words appearing in a text.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Bigrams represent a set of two consecutive words appearing in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c056005d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3561612634.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [34]\u001b[1;36m\u001b[0m\n\u001b[1;33m    bigrams function is called on tokenized words, as shown in the following example, to obtain bigrams.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Bigrams\n",
    "bigrams function is called on tokenized words, as shown in the following example, to obtain bigrams.\n",
    ">>> import nltk\n",
    ">>> s = 'Python is an awesome language.'\n",
    ">>> tokens = nltk.word_tokenize(s)\n",
    ">>> list(nltk.bigrams(tokens))\n",
    "[('Python', 'is'),\n",
    " ('is', 'an'),\n",
    " ('an', 'awesome'),\n",
    " ('awesome', 'language'),\n",
    " ('language', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6e06d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3557265904.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [35]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Computing Frequent Bigrams\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Computing Frequent Bigrams\n",
    "Now let's find out three frequently occurring bigrams, present in english-kjv collection of genesis corpus.\n",
    "\n",
    "Let's consider only those bigrams, whose words are having a length greater than 5.\n",
    "\n",
    ">>> eng_tokens = genesis.words('english-kjv.txt')\n",
    ">>> eng_bigrams = nltk.bigrams(eng_tokens)\n",
    ">>> filtered_bigrams = [ (w1, w2) for w1, w2 in eng_bigrams if len(w1) >=5 and len(w2) >= 5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df04d8c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1386055633.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [36]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Computing Frequent Bigrams\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Computing Frequent Bigrams\n",
    "After computing bi-grams, the following code computes frequency distribution and displays three most frequent bigrams.\n",
    ">>> eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    ">>> eng_bifreq.most_common(3)\n",
    "[(('their', 'father'), 19),\n",
    " (('lived', 'after'), 16),\n",
    " (('seven', 'years'), 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "Determining Frequent After Words\n",
    "Now let's see an example which determines the two most frequent words occurring after living are determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9873e9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 7), ('thing', 4)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "eng_tokens = genesis.words('english-kjv.txt')\n",
    "eng_bigrams = nltk.bigrams(eng_tokens)\n",
    "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
    "eng_cfd['living'].most_common(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35c940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d37874ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3633778092.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [38]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Generating Frequent Next Word\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Generating Frequent Next Word\n",
    "Now let's define a function named generate, which returns words occurring frequently after a given word.\n",
    ">>> def generate(cfd, word, n=5):\n",
    "...    n_words = []\n",
    "...    for i in range(n):\n",
    "...      n_words.append(word)\n",
    "...      word = cfd[word].max()\n",
    "...    return n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "613f4777",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (169841475.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [39]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Generating Most Frequent Next Word\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Generating Most Frequent Next Word\n",
    "After defining the function generate, it is called with eng_cfd and living parameters.\n",
    ">>> generate(eng_cfd, 'living')\n",
    "['living', 'creature', 'that', 'he', 'said']\n",
    "The output shows a word which occurs most frequently next to living is creature.\n",
    "Similarly that occurs more frequently after creature and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1457ca3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (906012235.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [40]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Similar to Bigrams, Trigrams refers to set of all three consecutive words appearing in text.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Trigrams\n",
    "Similar to Bigrams, Trigrams refers to set of all three consecutive words appearing in text.\n",
    ">>> s = 'Python is an awesome language.'\n",
    ">>> tokens = nltk.word_tokenize(s)\n",
    ">>> list(nltk.trigrams(tokens))\n",
    "[('Python', 'is', 'an'),\n",
    " ('is', 'an', 'awesome'),\n",
    " ('an', 'awesome', 'language'),\n",
    " ('awesome', 'language', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a775975",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2509785190.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [41]\u001b[1;36m\u001b[0m\n\u001b[1;33m    nltk also provides the function ngrams. It can be used to determine a set of all possible n consecutive words appearing in a text.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ngrams\n",
    "nltk also provides the function ngrams. It can be used to determine a set of all possible n consecutive words appearing in a text.\n",
    "\n",
    "The following example displays a list of four consecutive words appearing in the text s.\n",
    "\n",
    ">>> list(nltk.ngrams(tokens, 4))\n",
    "[('Python', 'is', 'an', 'awesome'),\n",
    " ('is', 'an', 'awesome', 'language'),\n",
    " ('an', 'awesome', 'language', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "282b3687",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (360587746.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [42]\u001b[1;36m\u001b[0m\n\u001b[1;33m    A collocation is a pair of words that occur together, very often.\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Collocations\n",
    "A collocation is a pair of words that occur together, very often.\n",
    "\n",
    "For example, red wine is a collocation.\n",
    "\n",
    "One characteristic of a collocation is that the words in it cannot be substituted with words having similar senses.\n",
    "\n",
    "For example, the combination maroon wine sounds odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6960fc10",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2824791298.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [43]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Generating Collocations\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Generating Collocations\n",
    "Now let's see how to generate collocations from text with the following example.\n",
    ">>> from nltk.corpus import genesis\n",
    ">>> tokens = genesis.words('english-kjv.txt')\n",
    ">>> gen_text = nltk.Text(tokens)\n",
    ">>> gen_text.collocations()\n",
    "said unto; pray thee; thou shalt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a283d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
      "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
      "seven years; shalt thou; little ones; living creature; creeping thing;\n",
      "savoury meat; thirty years; every beast\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "tokens = genesis.words('english-kjv.txt')\n",
    "gen_text = nltk.Text(tokens)\n",
    "gen_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55611e11",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b8ff4a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (680576306.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [46]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Stemming is a process of stripping affixes from words\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Stemming is a process of stripping affixes from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14dfc28c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3624332106.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [47]\u001b[1;36m\u001b[0m\n\u001b[1;33m    More often, you normalize text by converting all the words into lowercase. This will treat both words The and the as same.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Stemming\n",
    "More often, you normalize text by converting all the words into lowercase. This will treat both words The and the as same.\n",
    "\n",
    "With stemming, the words playing, played and play will be treated as single word, i.e. play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d33044c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3753025692.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [48]\u001b[1;36m\u001b[0m\n\u001b[1;33m    nltk comes with few stemmers.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Stemmers in nltk\n",
    "nltk comes with few stemmers.\n",
    "\n",
    "The two widely used stemmers are Porter and Lancaster stemmers.\n",
    "\n",
    "These stemmers have their own rules for string affixes.\n",
    "\n",
    "The following example demonstrates stemming of word builders using PorterStemmer.\n",
    "\n",
    ">>> from nltk import PorterStemmer\n",
    ">>> porter = nltk.PorterStemmer()\n",
    ">>> porter.stem('builders')\n",
    "builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0419d755",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3679871380.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [49]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Now let's see how to use LancasterStemmer and stem the word builders.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Stemmers in nltk\n",
    "Now let's see how to use LancasterStemmer and stem the word builders.\n",
    ">>> from nltk import LancasterStemmer\n",
    ">>> lancaster = LancasterStemmer()\n",
    ">>> lancaster.stem('builders')\n",
    "build\n",
    "Lancaster Stemmer returns build whereas Porter Stemmer returns builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e93b03ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3509677418.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [50]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Normalizing with Stemming\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Normalizing with Stemming\n",
    "Let's consider the text collection, text1.\n",
    "\n",
    "Let's first determine the number of unique words present in original text1.\n",
    "\n",
    "Then normalize the text by converting all the words into lower case and again determine the number of unique words.\n",
    "\n",
    ">>> from nltk.book import *\n",
    ">>> len(set(text1))\n",
    "19317\n",
    ">>> lc_words = [ word.lower() for word in text1] \n",
    ">>> len(set(lc_words))\n",
    "17231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bf87cea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2947908657.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [51]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Normalizing with Stemming\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Normalizing with Stemming\n",
    "Now let's further normalize text1 with Porter Stemmer.\n",
    ">>> from nltk import PorterStemmer\n",
    ">>> porter = PorterStemmer()\n",
    ">>> p_stem_words = [porter.stem(word) for word in set(lc_words) ]\n",
    ">>> len(set(p_stem_words))\n",
    "10927\n",
    "The above output shows that, after normalising with Porter Stemmer, the text1 collection has 10927 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6802445b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1788060477.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [52]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Normalising with Stemming\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Normalising with Stemming\n",
    "Now let's normalise with Lancaster stemmer and determine the unique words of text1.\n",
    ">>> from nltk import LancasterStemmer\n",
    ">>> lancaster = LancasterStemmer()\n",
    ">>> l_stem_words = [lancaster.stem(word) for word in set(lc_words) ]\n",
    ">>> len(set(l_stem_words))\n",
    "9036\n",
    "Applying Lancaster Stemmer to text1 collection resulted in 9036 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b636515f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2721452265.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [53]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Understanding Lemma\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Understanding Lemma\n",
    "Lemma is a lexical entry in a lexical resource such as word dictionary.\n",
    "\n",
    "You can find multiple Lemma's with the same spelling. These are known as homonyms.\n",
    "\n",
    "For example, consider the two Lemma's listed below, which are homonyms.\n",
    "\n",
    "1. saw [verb] - Past tense of see\n",
    "2. saw [noun] - Cutting instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27c9f165",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (72590613.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [54]\u001b[1;36m\u001b[0m\n\u001b[1;33m    nltk comes with WordNetLemmatizer. This lemmatizer removes affixes only if the resulting word is found in lexical resource, Wordnet.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Lemmatization\n",
    "nltk comes with WordNetLemmatizer. This lemmatizer removes affixes only if the resulting word is found in lexical resource, Wordnet.\n",
    ">>> wnl = nltk.WordNetLemmatizer()\n",
    ">>> wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words) ]\n",
    ">>> len(set(wnl_stem_words))\n",
    "15168 \n",
    "WordNetLemmatizer is majorly used to build a vocabulary of words, which are valid Lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70f4b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "print(porter.stem('lying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0304df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print(lancaster.stem('lying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec995e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceremoni\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "print(porter.stem('ceremony'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05305c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\silri\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print(wnl.lemmatize('women'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106e02f",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec4750",
   "metadata": {},
   "source": [
    "The method of categorizing words into their parts of speech and then labeling them respectively is called POS Tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef2c10",
   "metadata": {},
   "source": [
    "#### POS Tagger\n",
    "\n",
    "A POS Tagger processes a sequence of words and tags a part of speech to each word.\n",
    "\n",
    "pos_tag is the simplest tagger available in nltk.\n",
    "\n",
    "The below example shows usage of pos_tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "885a3bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafe3cc",
   "metadata": {},
   "source": [
    "The words Python, is and awesome are tagged to Proper Noun (NNP), Present Tense Verb (VB), and adjective (JJ) respectively.\n",
    "\n",
    "You can read more about the pos tags with the below help command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4e734a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127798ce",
   "metadata": {},
   "source": [
    "To know about a specific tag like JJ, use the below-shown expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c65b1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0301d",
   "metadata": {},
   "source": [
    "#### Tagging Text\n",
    "Constructing a list of tagged words from a string is possible.\n",
    "\n",
    "A tagged word or token is represented in a tuple, having the word and the tag.\n",
    "\n",
    "In the input text, each word and tag are separated by /."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e628f220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'VB'), ('awesome', 'JJ'), ('.', '.')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Python/NN is/VB awesome/JJ ./.'\n",
    "[ nltk.tag.str2tuple(word) for word in text.split() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f07597",
   "metadata": {},
   "source": [
    "#### Tagged Corpora\n",
    "Many of the text corpus available in nltk, are already tagged to their respective parts of speech.\n",
    "\n",
    "tagged_words method can be used to obtain tagged words of a corpus.\n",
    "\n",
    "The following example fetches tagged words of brown corpus and displays few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50de7fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged = brown.tagged_words()\n",
    "brown_tagged[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3a44c",
   "metadata": {},
   "source": [
    "#### DefaultTagger\n",
    "DefaultTagger assigns a specified tag to every word or token of given text.\n",
    "\n",
    "An example of tagging NN tag to all words of a sentence, is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7383e7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NN'), ('is', 'NN'), ('awesome', 'NN'), ('.', 'NN')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b344e0",
   "metadata": {},
   "source": [
    "#### Lookup Tagger\n",
    "You can define a custom tagger and use it to tag words present in any text.\n",
    "\n",
    "The below-shown example defines a dictionary defined_tags, with three words and their respective tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "43c4dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = 'Python is awesome.'\n",
    "words = nltk.word_tokenize(text)\n",
    "defined_tags = {'is':'BEZ', 'over':'IN', 'who': 'WPS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153173b2",
   "metadata": {},
   "source": [
    "The example further defines a UnigramTagger with the defined dictionary and uses it to predict tags of words in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a842d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', None), ('is', 'BEZ'), ('awesome', None), ('.', None)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "baseline_tagger.tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c8323",
   "metadata": {},
   "source": [
    "#### Unigram Tagger\n",
    "\n",
    "- UnigramTagger provides you the flexibility to create your taggers.\n",
    "- Unigram taggers are built based on statistical information. i.e., they tag each word or token to most likely tag for that particular word.\n",
    "\n",
    "- You can build a unigram tagger through a process known as training.\n",
    "\n",
    "- Then use the tagger to tag words in a test set and evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a3e23",
   "metadata": {},
   "source": [
    "Let's consider the tagged sentences of brown corpus collections, associated with government genre.\n",
    "\n",
    "Let's also compute the training set size, i.e., 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d59b483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3032\n",
      "2425\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='government')\n",
    "brown_sents = brown.sents(categories='government')\n",
    "print(len(brown_sents))\n",
    "train_size = int(len(brown_sents)*0.8)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82c1d748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799495586380832"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = brown_tagged_sents[:train_size]\n",
    "test_sents = brown_tagged_sents[train_size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.accuracy(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530376a",
   "metadata": {},
   "source": [
    "unigram_tagger is built by passing trained tagged sentences as argument to UnigramTagger.\n",
    "\n",
    "The built unigram_tagger is further evaluated with test sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc057cf",
   "metadata": {},
   "source": [
    "The following code snippet shows tagging words of a sentence, taken from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "544a7f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('first', 'OD'),\n",
       " ('step', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('a', 'AT'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('self', None),\n",
       " ('study', 'NN'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('faculty', None),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('outside', 'IN'),\n",
       " ('consultants', 'NNS'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'AT'),\n",
       " ('combination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('two', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.tag(brown_sents[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981d725",
   "metadata": {},
   "source": [
    "#### Summary on NLP with Python\n",
    "\n",
    "In this course, you have mastered the following:\n",
    "\n",
    "- Tokenizing text using functions word_tokenize and sent_tokenize.\n",
    "\n",
    "- Computing Frequencies with FreqDist and ConditionalFreqDist.\n",
    "\n",
    "- Generating Bigrams and collocations with bigrams and collocations.\n",
    "\n",
    "- Stemming word affixes using PorterStemmer and LancasterStemmer.\n",
    "\n",
    "- Tagging words to their parts of speech using pos_tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2745e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
